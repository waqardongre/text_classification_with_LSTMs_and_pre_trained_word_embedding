{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIf725o-Z2XW"
      },
      "source": [
        "## References : \n",
        "1. https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
        "2. https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7_YLiMAZ2Xh"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eH-R13geZ2Xl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import LSTM\r\n",
        "\r\n",
        "import math\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3zoddqlZ2Xp"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this project, we show how to train a text classification model that uses pre-trained\n",
        "word embeddings.\n",
        "\n",
        "We'll work with the AclImdb dataset, a set of total 25,000 with positive 12,500 positive and 12,500 negative movie reviews.\n",
        "\n",
        "For the pre-trained word embeddings, we'll use\n",
        "[GloVe embeddings](http://nlp.stanford.edu/projects/glove/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment below 2 lines if you are using google drive.\r\n",
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjOfI1v6Z2Xr"
      },
      "source": [
        "## Download the AclImdb data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download data from here: 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz' and place in your root of project folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### uncomment and run script in below cell only one time to extract this in the root of your project folder. Comment after executed. Update the paths according to your project path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZcNzuM9slOd3"
      },
      "outputs": [],
      "source": [
        "#!tar -xzvf '/content/drive/MyDrive/DS/nlp_movie_ratings/aclImdb_v1.tar.gz' -C '/content/drive/MyDrive/DS/nlp_movie_ratings/root'     #[run this cell to extract tar.gz files]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set the path to the folder where data is extracted in your project folder in this main_path variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Asz_1JLplLw_"
      },
      "outputs": [],
      "source": [
        "#main_path = \"/content/drive/MyDrive/DS/nlp_movie_ratings/root/aclImdb/\" # wenoff\r\n",
        "main_path = \"/content/drive/MyDrive/DS/nlp_movie_ratings/root/aclImdb/\" # wazu\r\n",
        "#main_path = \"/content/drive/MyDrive/DSs/nlp_movie_ratings/root/aclImdb/\" # dowa and jale\r\n",
        "#main_path = \"/content/drive/MyDrive/datasets/nlp_movie_ratings/root/aclImdb/\" # wado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA-l7gr_Z2Xt"
      },
      "source": [
        "## Let's take a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgsZMRzCZ2Xu",
        "outputId": "b3134912-289b-421c-eedb-130168e16b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of directories: 8\n",
            "Directory names: ['labeledBow.feat', 'neg', 'pos', 'unsup', 'unsupBow.feat', 'urls_neg.txt', 'urls_pos.txt', 'urls_unsup.txt']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path(main_path).parent / \"aclImdb/train\"\n",
        "dirnames = os.listdir(data_dir)\n",
        "print(\"Number of directories:\", len(dirnames))\n",
        "print(\"Directory names:\", sorted(dirnames))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1cN5JDVZ2Xw"
      },
      "source": [
        "Here's a example of what one file contains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKrB-S59Z2Xy",
        "outputId": "92a808a8-7426-4a00-fbb2-497bde16bc2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
          ]
        }
      ],
      "source": [
        "print(open(data_dir / \"pos\" / \"0_9.txt\").read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOkFzeAIZ2Xz"
      },
      "source": [
        "As you can see, there are header lines that are leaking the file's category, either\n",
        "explicitly (the first line is literally the category name), or implicitly, e.g. via the\n",
        "`Organization` filed. Let's get rid of the headers:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5y1Rteg59mG"
      },
      "source": [
        "Setting total data size from total 25000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7Mu1RZse59Uf"
      },
      "outputs": [],
      "source": [
        "no_of_rows = 2500 # for both class - total 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaFr71INZ2X0",
        "outputId": "4e33db67-18bc-4aa3-ebc8-6541a5316bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing pos, 2500 files found\n",
            "Processing neg, 2500 files found\n",
            "Classes: ['pos', 'neg']\n",
            "Number of samples: 5000\n"
          ]
        }
      ],
      "source": [
        "samples = []\n",
        "labels = []\n",
        "class_names = dirnames = ['pos', 'neg']\n",
        "class_index = 0\n",
        "for dirname in dirnames:\n",
        "    dirpath = data_dir / dirname\n",
        "    fnames = os.listdir(dirpath)[:no_of_rows]\n",
        "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
        "    for fname in fnames:\n",
        "        fpath = dirpath / fname\n",
        "        f = open(fpath, encoding=\"latin-1\")\n",
        "        content = f.read()\n",
        "        samples.append(content)\n",
        "        labels.append(class_index)\n",
        "    class_index += 1\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of samples:\", len(samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qvL465cZ2X2"
      },
      "source": [
        "## Shuffle and split the data into training & validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dyVeYTF0Z2X3"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data\n",
        "seed = 9\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(samples)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Extract a training & validation split\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(samples))\n",
        "train_samples = samples[:-num_validation_samples]\n",
        "val_samples = samples[-num_validation_samples:]\n",
        "train_labels = labels[:-num_validation_samples]\n",
        "val_labels = labels[-num_validation_samples:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgH2HuoHZ2X4"
      },
      "source": [
        "## Create a vocabulary index by using imdb vocabulary given with the dataset\n",
        "\n",
        "Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n",
        "Later, we'll use the same layer instance to vectorize the samples.\n",
        "\n",
        "Our layer will only consider the top 5,000 words, and will truncate or pad sequences to\n",
        "be actually 200 tokens long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OFCZogR5Z2X5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=5000, output_sequence_length=200, vocabulary = main_path + \"imdb.vocab\")\n",
        "#text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
        "#vectorizer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj8RKvFjZ2X5"
      },
      "source": [
        "You can retrieve the computed vocabulary used via `vectorizer.get_vocabulary()`. Let's\n",
        "print the top 5 words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DzN7iBhZ2X6",
        "outputId": "385ef58c-350c-4481-bc2c-4ecc4769be12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'and', 'a']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizer.get_vocabulary()[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y304YbzZ2X7"
      },
      "source": [
        "Let's vectorize a test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uRqXL7pyZ2X7"
      },
      "outputs": [],
      "source": [
        "#output = vectorizer([[\"the cat sat on the mat\"]])\n",
        "#output.numpy()[0, :6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKwhBt6YZ2X8"
      },
      "source": [
        "As you can see, \"the\" gets represented as \"2\". Why not 0, given that \"the\" was the first\n",
        "word in the vocabulary? That's because index 0 is reserved for padding and index 1 is\n",
        "reserved for \"out of vocabulary\" tokens.\n",
        "\n",
        "Here's a dict mapping words to their indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "118H6wn1Z2X8"
      },
      "outputs": [],
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVAtIFW2Z2X9"
      },
      "source": [
        "As you can see, we obtain the same encoding as above for our test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtPdIuKnDZTn",
        "outputId": "b8273a51-19a6-44df-cf5a-25250f33b8da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "685"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_index[\"word\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Imp9-tRZ2X9",
        "outputId": "b3df8982-e1d8-479b-a6d3-fb0efcfb8818"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 1080, 1777, 20, 2, 12332]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "[word_index[w] for w in test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving vectorizer for deployment\r\n",
        "\r\n",
        "# Reference https://stackoverflow.com/questions/65103526/how-to-save-textvectorization-to-disk-in-tensorflow\r\n",
        "\r\n",
        "# Pickle the config and weights\r\n",
        "pickle.dump({'config': vectorizer.get_config(),\r\n",
        "             'weights': vectorizer.get_weights()}\r\n",
        "            , open(main_path + \"vectorizer.pkl\", \"wb\"))\r\n",
        "\r\n",
        "# Later you can unpickle and use \r\n",
        "# `config` to create object and \r\n",
        "# `weights` to load the trained weights. \r\n",
        "\r\n",
        "# from_disk = pickle.load(open(main_path + \"vectorizer.pkl\", \"rb\"))\r\n",
        "# vectorizer = TextVectorization.from_config(from_disk['config'])\r\n",
        "# vectorizer.set_weights(from_disk['weights'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL9yyfYfZ2X-"
      },
      "source": [
        "## Load pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRsjr3eGZ2X-"
      },
      "source": [
        "Let's download pre-trained GloVe embeddings (a 822M zip file).\n",
        "\n",
        "You'll need to run the following commands:\n",
        "\n",
        "```\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gnYSBzVFtEr2"
      },
      "outputs": [],
      "source": [
        "# one time only\n",
        "#!unzip \"/content/drive/MyDrive/DSs/21a/glove.6B.zip\" -d \"/content/drive/MyDrive/DSs/21a/glove6B/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2lgjhfxZ2X_"
      },
      "source": [
        "The archive contains text-encoded vectors of various sizes: 50-dimensional,\n",
        "100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n",
        "\n",
        "Let's make a dict mapping words (strings) to their NumPy vector representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_bCW8uIZ2X_",
        "outputId": "1d58b183-aa5b-470b-f891-4200205b739d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "path_to_glove_file = \"/content/drive/MyDrive/DS/21a/glove6B/glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKG0o1fxZ2YA"
      },
      "source": [
        "Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n",
        "`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n",
        "vector for the word of index `i` in our `vectorizer`'s vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCF1QrnbZ2YA",
        "outputId": "509cd86f-3376-4002-8045-1c6b514a33ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 62596 words (26933 misses)\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dajYaHrjZ2YB"
      },
      "source": [
        "Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n",
        "\n",
        "Note that we set `trainable=False` so as to keep the embeddings fixed (we don't want to\n",
        "update them during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0adau9i-dSY",
        "outputId": "97b08ffc-6abd-4f83-b362-69d5966d2a32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(89531, 100)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_tokens, embedding_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dWRwyZsCZ2YB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yiKvOnDZ2YB"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "A simple 1D convnet with global max pooling and a classifier at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kOZFWO9aZ2YC"
      },
      "outputs": [],
      "source": [
        "def create_rnn(cell_units = 50, dropout = 0.2):\n",
        "    # Initialising the RNN\n",
        "    regressor = Sequential()\n",
        "\n",
        "    regressor.add(embedding_layer)\n",
        "\n",
        "    # Adding a 1st LSTM layer and some Dropout regularisation\n",
        "    regressor.add(LSTM(units = cell_units, return_sequences = True, dropout=dropout, recurrent_dropout=dropout))\n",
        "\n",
        "    # Adding a 2nd LSTM layer and some Dropout regularisation\n",
        "    regressor.add(LSTM(units = cell_units, return_sequences = True, dropout=dropout, recurrent_dropout=dropout))\n",
        "\n",
        "    # Adding 3rd LSTM layer and some Dropout regularisation\n",
        "    regressor.add(LSTM(units = cell_units, dropout=dropout, recurrent_dropout=dropout))\n",
        "\n",
        "    # Adding the output layer\n",
        "    regressor.add(Dense(units = 1, activation='sigmoid'))\n",
        "\n",
        "    return regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKmbSq_PZ2YC"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays\n",
        "are right-padded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eYo86bSvZ2YD"
      },
      "outputs": [],
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdsBdKDK8d4h",
        "outputId": "b0be5457-84b1-4409-a5a5-f2f2db830dac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4000, 200), (4000,), (1000, 200), (1000,))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5SCeV80uybB",
        "outputId": "a7eea534-a1f3-47ba-9864-cab877fe2e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ],
      "source": [
        "units = 20\n",
        "dropz_out = 0.1\n",
        "model = create_rnn(units, dropz_out)\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg0eA5YB04Kq",
        "outputId": "83db5512-cbdb-4ba2-c818-94e0937eef06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "5/5 [==============================] - 10s 1s/step - loss: 0.6839 - accuracy: 0.5680 - val_loss: 0.6725 - val_accuracy: 0.6740\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.6583 - accuracy: 0.6885 - val_loss: 0.6335 - val_accuracy: 0.6960\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5930 - accuracy: 0.7440 - val_loss: 0.5708 - val_accuracy: 0.7470\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5305 - accuracy: 0.7732 - val_loss: 0.5600 - val_accuracy: 0.7580\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5219 - accuracy: 0.7832 - val_loss: 0.5651 - val_accuracy: 0.7190\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5472 - accuracy: 0.7385 - val_loss: 0.5619 - val_accuracy: 0.7300\n",
            "Epoch 7/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5207 - accuracy: 0.7725 - val_loss: 0.5824 - val_accuracy: 0.7610\n",
            "Epoch 8/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5307 - accuracy: 0.7780 - val_loss: 0.5453 - val_accuracy: 0.7400\n",
            "Epoch 9/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5220 - accuracy: 0.7602 - val_loss: 0.5514 - val_accuracy: 0.7240\n",
            "Epoch 10/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5068 - accuracy: 0.7740 - val_loss: 0.5250 - val_accuracy: 0.7740\n",
            "Epoch 11/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5022 - accuracy: 0.7880 - val_loss: 0.5201 - val_accuracy: 0.7730\n",
            "Epoch 12/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4957 - accuracy: 0.7815 - val_loss: 0.5309 - val_accuracy: 0.7490\n",
            "Epoch 13/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4920 - accuracy: 0.7857 - val_loss: 0.5207 - val_accuracy: 0.7800\n",
            "Epoch 14/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5066 - accuracy: 0.7840 - val_loss: 0.5293 - val_accuracy: 0.7750\n",
            "Epoch 15/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4996 - accuracy: 0.7887 - val_loss: 0.5262 - val_accuracy: 0.7580\n",
            "Epoch 16/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.5011 - accuracy: 0.7815 - val_loss: 0.5052 - val_accuracy: 0.7720\n",
            "Epoch 17/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4850 - accuracy: 0.7915 - val_loss: 0.5098 - val_accuracy: 0.7820\n",
            "Epoch 18/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4779 - accuracy: 0.7940 - val_loss: 0.5088 - val_accuracy: 0.7670\n",
            "Epoch 19/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4791 - accuracy: 0.7885 - val_loss: 0.5165 - val_accuracy: 0.7620\n",
            "Epoch 20/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4750 - accuracy: 0.7872 - val_loss: 0.5015 - val_accuracy: 0.7730\n",
            "Epoch 21/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4782 - accuracy: 0.7795 - val_loss: 0.5074 - val_accuracy: 0.7530\n",
            "Epoch 22/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4764 - accuracy: 0.7782 - val_loss: 0.5033 - val_accuracy: 0.7660\n",
            "Epoch 23/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4653 - accuracy: 0.7952 - val_loss: 0.4913 - val_accuracy: 0.7810\n",
            "Epoch 24/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4623 - accuracy: 0.7977 - val_loss: 0.4946 - val_accuracy: 0.7820\n",
            "Epoch 25/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4645 - accuracy: 0.8033 - val_loss: 0.5022 - val_accuracy: 0.7930\n",
            "Epoch 26/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4659 - accuracy: 0.8018 - val_loss: 0.4812 - val_accuracy: 0.7750\n",
            "Epoch 27/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4673 - accuracy: 0.7885 - val_loss: 0.4869 - val_accuracy: 0.7630\n",
            "Epoch 28/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4575 - accuracy: 0.7995 - val_loss: 0.4843 - val_accuracy: 0.7880\n",
            "Epoch 29/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4613 - accuracy: 0.8027 - val_loss: 0.4767 - val_accuracy: 0.7790\n",
            "Epoch 30/30\n",
            "5/5 [==============================] - 5s 1s/step - loss: 0.4543 - accuracy: 0.8012 - val_loss: 0.4719 - val_accuracy: 0.7910\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9bb875e850>"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fix random seed for reproducibility\n",
        "np.random.seed(9)\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(main_path + \"model_checkpoints/text_classification_lstm_4.h5\", save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=800, epochs=30, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hRau_lIkrE8"
      },
      "source": [
        "## Part 3 - Making the predictions and visualising the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgJO6qEDksxD"
      },
      "source": [
        "### Getting the real movie ratings of test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "M_XY-5uVrICp"
      },
      "outputs": [],
      "source": [
        "real_ratings = y_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrvrLblxkz42"
      },
      "source": [
        "### Getting the predicted movie ratings of test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "emikTvUpk3Ck"
      },
      "outputs": [],
      "source": [
        "predicted_ratings_decimals = model.predict(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wANmJWRhwn3s",
        "outputId": "94603981-8978-4b30-b858-6ae723d3399b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 79.10%\n"
          ]
        }
      ],
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(x_val, y_val, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5LMNpsNAdin"
      },
      "source": [
        "### Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "1mFmpe_NAdLW"
      },
      "outputs": [],
      "source": [
        "#model.save(main_path + 'LSTM_h5_model_4_acc.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text_classification_using_pretrained_word_embeddings_by_Waqar_Dongre.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit (conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}